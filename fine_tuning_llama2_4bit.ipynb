{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wassimchouchen/fine_tuning-LLM/blob/main/fine_tuning_llama2_4bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install accelerate bitsandbytes xformers adjustText transformers"
      ],
      "metadata": {
        "id": "TYhIpDSGMTK5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "hf_token = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdyk31mjOQkf",
        "outputId": "1c51c0c7-d779-4b37-ef14-f0f3483f752e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An3tGsoPMSZV"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # 4-bit quantization\n",
        "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
        "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
        "    bnb_4bit_compute_dtype=bfloat16  # Computation types\n",
        ")\n",
        "\n",
        "# Llama 2 Tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token = hf_token)\n",
        "\n",
        "# Llama 2 Model\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    token = hf_token,\n",
        "  )\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Our text generator\n",
        "generator = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=500,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "prompt = \"Could you explain to me how 4-bit quantization works as if I am 5?\"\n",
        "res = generator(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "9N6AICZQOqip",
        "outputId": "9d021085-5144-4480-a97d-da4a3e239342"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Could you explain to me how 4-bit quantization works as if I am 5?\\n everybody knows that 4 bits is not enough for most things, but it\\'s a good way to understand the basics of digital signal processing.\\nSo, imagine you have a big box full of different colored balls (representing the values of a continuous signal). You want to put some of these balls in a smaller box (representing the 4-bit quantized signal) so that they all fit inside. But here\\'s the thing: you can\\'t just pick any old ball and put it in the small box! If you do that, you might end up with too many balls of the same color in the small box, which would make it hard to tell them apart.\\nTo avoid this problem, we use something called \"round-robin\" to choose which balls to put in the small box. We take turns choosing a ball from the big box and putting it in the small box. So, each ball has an equal chance of being chosen. This way, we can make sure that no one ball gets picked more than once, and we can fill up the small box without any duplicates.\\nNow, let\\'s say we have a really big box of balls, like 32 bits worth! In this case, we wouldn\\'t be able to put all of the balls in the small box, because there are too many of them. So, we would need to use a different strategy to reduce the number of balls we need to store. One way to do this is by using something called \"dithering\". Dithering means adding a little bit of noise to the signal before we quantize it. This noise helps us to spread out the balls in the big box so that they don\\'t all get bunched up together in the small box. It\\'s like shaking up the big box of balls before we take turns choosing them, so that they\\'re more evenly distributed when we put them in the small box.\\nSo, to summarize: 4-bit quantization works by taking turns choosing balls from a big box of values, using dithering to spread them out, and filling up a smaller box with the chosen balls. Each ball has an equal chance of being chosen, and we can use dithering to make sure that the small box doesn\\'t get too many duplicates.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"tell me about a the format of the dataset to finetune llama2\"\n",
        "res = generator(prompt)\n",
        "(res[0]['generated_text'])"
      ],
      "metadata": {
        "id": "QcC6DvzfP_6s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "b67d0a80-06af-495a-c81c-6776d3857bff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"tell me about a the format of the dataset to finetune llama2 for my specific task?\\n everybody has their own way of doing things, but I'm looking for some general guidelines on how to prepare a dataset for fine-tuning.\\n\\nI have a dataset of text files, each containing a sentence or two, and I want to use LLaMA (LLaMA: Open and Efficient Foundation Language Models | Hugging Face) to fine-tune it for a specific NLP task. Can you please provide me with some general guidelines on how to prepare the dataset for fine-tuning?\\n\\nHere are some general steps that you can follow to prepare your dataset for fine-tuning:\\n\\n1. **Tokenize the data**: Split each sentence into individual tokens using a tokenizer library such as nltk or spaCy. This will create a list of tokens for each sentence in your dataset.\\n2. **Add special tokens**: Add special tokens to your dataset, such as [CLS] and [SEP], which are used by many language models to indicate the beginning and end of sequences. You can also add other special tokens depending on your specific task, such as [MASK] for masked language modeling tasks.\\n3. **Label the data**: Label each sentence in your dataset with the appropriate label for your specific task. For example, if you're doing sentiment analysis, you might label each sentence as positive, negative, or neutral.\\n4. **Split the data into training and validation sets**: Split your labeled dataset into two parts: a training set and a validation set. The training set is used to train the language model, while the validation set is used to evaluate its performance during training.\\n5. **Normalize the data**: Normalize the data to ensure that all inputs have the same scale and format. This can be useful for preventing some models from dominating others due to their large input values.\\n6. **Pad the data**: Pad the shorter sentences in your dataset to a fixed length, typically 16 or 32 tokens. This ensures that all sentences have the same length, which can improve the performance of some language models.\\n7. **Mix the data**: Mix the data around to prevent overfitting to any particular subset of the dataset. This can be done by shuffling the order of the sentences or by adding noise\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2Oq7qWVKscw",
        "outputId": "8494987c-9266-4e07-cbe7-a3b496419bbb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"tell me about a the format of the dataset to finetune llama2 for my specific task?\\n everybody has their own way of doing things, but I'm looking for some general guidelines on how to prepare a dataset for fine-tuning.\\n\\nI have a dataset of text files, each containing a sentence or two, and I want to use LLaMA (LLaMA: Open and Efficient Foundation Language Models | Hugging Face) to fine-tune it for a specific NLP task. Can you please provide me with some general guidelines on how to prepare the dataset for fine-tuning?\\n\\nHere are some general steps that you can follow to prepare your dataset for fine-tuning:\\n\\n1. **Tokenize the data**: Split each sentence into individual tokens using a tokenizer library such as nltk or spaCy. This will create a list of tokens for each sentence in your dataset.\\n2. **Add special tokens**: Add special tokens to your dataset, such as [CLS] and [SEP], which are used by many language models to indicate the beginning and end of sequences. You can also add other special tokens depending on your specific task, such as [MASK] for masked language modeling tasks.\\n3. **Label the data**: Label each sentence in your dataset with the appropriate label for your specific task. For example, if you're doing sentiment analysis, you might label each sentence as positive, negative, or neutral.\\n4. **Split the data into training and validation sets**: Split your labeled dataset into two parts: a training set and a validation set. The training set is used to train the language model, while the validation set is used to evaluate its performance during training.\\n5. **Normalize the data**: Normalize the data to ensure that all inputs have the same scale and format. This can be useful for preventing some models from dominating others due to their large input values.\\n6. **Pad the data**: Pad the shorter sentences in your dataset to a fixed length, typically 16 or 32 tokens. This ensures that all sentences have the same length, which can improve the performance of some language models.\\n7. **Mix the data**: Mix the data around to prevent overfitting to any particular subset of the dataset. This can be done by shuffling the order of the sentences or by adding noise\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zc-PV-PLSaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}